import json 
from torch import nn 
import torch 
import triton_python_backend_utils as pb_utils 
from pathlib import Path 
from transformers import BertModel 
 
class BERT_Arch(nn.Module): 
    def __init__(self): 
        super(BERT_Arch, self).__init__() 
        self.bert = BertModel.from_pretrained('bert-base-uncased') 
        self.dropout = nn.Dropout(0.1) 
        self.relu =  nn.ReLU() 
        self.fc1 = nn.Linear(768,512) 
        self.fc2 = nn.Linear(512,2) 
        self.softmax = nn.LogSoftmax(dim=1) 
         
    #define the forward pass 
    def forward(self, sent_id, mask): 
        #pass the inputs to the model   
        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False) 
        x = self.fc1(cls_hs) 
        x = self.relu(x) 
        x = self.dropout(x) 
        # output layer 
        x = self.fc2(x) 
        # apply softmax activation 
        x = self.softmax(x) 
        return x 
 
 
class TritonPythonModel: 
    """Your Python model must use the same class name. Every Python model 
    that is created must have "TritonPythonModel" as the class name. 
    """ 
    def initialize(self, args): 
        """`initialize` is called only once when the model is being loaded. 
        Implementing `initialize` function is optional. This function allows 
        the model to intialize any state associated with this model. 
        Parameters 
        ---------- 
        args : dict 
          Both keys and values are strings. The dictionary keys and values are: 
          * model_config: A JSON string containing the model configuration 
          * model_instance_kind: A string containing model instance kind 
          * model_instance_device_id: A string containing model instance device ID 
          * model_repository: Model repository path 
          * model_version: Model version 
          * model_name: Model name 
        """ 
 
        # You must parse model_config. JSON string is not parsed here 
        self.model_config = model_config = json.loads(args['model_config']) 
        self.model_instance_device_id  = json.loads(args['model_instance_device_id']) 
 
 
        self.device = torch.device("cuda:{}".format(self.model_instance_device_id) if torch.cuda.is_available() else "cpu") 
         
        model = BERT_Arch() 
         
        # Load saved model 
        m_p = Path(__file__).with_name('model.pt') 
        model.load_state_dict(torch.load(m_p))     
         
        model = model.eval() 
        self.model = model.to(self.device) 
 
    def execute(self, requests): 
        """`execute` must be implemented in every Python model. `execute` 
        function receives a list of pb_utils.InferenceRequest as the only 
        argument. This function is called when an inference is requested 
        for this model. Depending on the batching configuration (e.g. Dynamic 
        Batching) used, `requests` may contain multiple requests. Every 
        Python model, must create one pb_utils.InferenceResponse for every 
        pb_utils.InferenceRequest in `requests`. If there is an error, you can 
        set the error argument when creating a pb_utils.InferenceResponse. 
        Parameters 
        ---------- 
        requests : list 
          A list of pb_utils.InferenceRequest 
        Returns 
        ------- 
        list 
          A list of pb_utils.InferenceResponse. The length of this list must 
          be the same as `requests` 
        """ 
 
 
        responses = [] 
 
        # Every Python backend must iterate over everyone of the requests 
        # and create a pb_utils.InferenceResponse for each of them. 
        for request in requests: 
            # Get INPUT0 
            input_ids = pb_utils.get_input_tensor_by_name(request, "input_ids") 
            attention_mask = pb_utils.get_input_tensor_by_name(request, "attention_mask") 
            ### Wont Need below conversion in newer releases 
            ### see PR https://github.com/triton-inference-server/python_backend/pull/62 
            input_ids = torch.Tensor(input_ids.as_numpy()).long().to(self.device) 
            attention_mask = torch.Tensor(attention_mask.as_numpy()).long().to(self.device) 
             
            with torch.no_grad(): 
                outputs = self.model(input_ids, attention_mask) 
                conf, preds = torch.max(outputs, dim=1) 
 
            # Create output tensors. You need pb_utils.Tensor 
            # objects to create pb_utils.InferenceResponse. 
            out_tensor_0 = pb_utils.Tensor("preds", preds.cpu().numpy()) 
             
            # Create InferenceResponse. You can set an error here in case 
            # there was a problem with handling this inference request. 
            # Below is an example of how you can set errors in inference 
            # response: 
            # 
            # pb_utils.InferenceResponse( 
            #    output_tensors=..., TritonError("An error occured")) 
            inference_response = pb_utils.InferenceResponse(output_tensors=[out_tensor_0]) 
            responses.append(inference_response) 
 
        # You should return a list of pb_utils.InferenceResponse. Length 
        # of this list must match the length of `requests` list. 
        return responses 
 
    def finalize(self): 
        """`finalize` is called only once when the model is being unloaded. 
        Implementing `finalize` function is optional. This function allows 
        the model to perform any necessary clean ups before exit. 
        """ 
        print('Cleaning up...') 
